{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T15:37:33.794298Z",
     "start_time": "2024-11-18T15:37:17.996201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"image-to-text\", model=\"microsoft/trocr-large-handwritten\")"
   ],
   "id": "748e09e1bc1b8553",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amaljoe/Desktop/Workspace/IITB/NLP/OCR_with_LLMs/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": false,\n",
      "  \"transformers_version\": \"4.46.2\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 1024,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"transformers_version\": \"4.46.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-large-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# load image from the IAM database\n",
    "url = 'https://storage.googleapis.com/kagglesdsdata/datasets/1437621/2378970/dataset/a01-003-s00-01.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20241113%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20241113T065013Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=2411cbef74985b4e8aae82aaf048718331f1f32f703838e9041d42b3061f60ca8eb1d6eeef276dff19711dae5a18d43c7ea6f55c0fb6303353c1631ac5c401f70ca59af94ec3961be04e23c8bdbac9eafa5f0ffe63c32b6c3056d3bb9b173d049c0a8eb6b2f61bbc26552cda97a2d2c329e55ec26b7e86488d02bc0297b9b4c52324749bdb6c4d2c0d01fa9bf8ee16254b26d16ddbaecc0d8b206ed3f7943125db9fcd135423e905127ef02b8e62f297a2667aaef7a0c87a1d02309e03dedd4fe30b3a0098029dfb137dcbee266d993bd4fb57af512899dbf264fa0d0647bd173029e48175a7cf589fad9f12cea4d120d5a2bf975f59f14eba3b138bca5ac99a'\n",
    "filepath = 'data/ss2.png'\n",
    "image = Image.open(filepath).convert(\"RGB\")\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\n",
    "model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\n",
    "pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values"
   ],
   "id": "ed4d8c68a09385df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "generated_ids = model.generate(pixel_values)\n",
    "generated_ids"
   ],
   "id": "da8a35e63c427334"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(generated_text)"
   ],
   "id": "785faaac77cd6ca4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T16:09:53.226155Z",
     "start_time": "2024-11-18T16:09:51.341911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def infer_text(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "    logits = model.generate(pixel_values)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return generated_text\n",
    "\n",
    "infer_text('data/ss2.png')"
   ],
   "id": "675c3f8600b9caca",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50265])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T15:41:25.054158Z",
     "start_time": "2024-11-18T15:41:10.803480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "def get_all_files(folder_path):\n",
    "    files = []\n",
    "    for root, dirs, filenames in os.walk(folder_path):\n",
    "        for filename in filenames:\n",
    "            files.append(os.path.join(root, filename))\n",
    "    return files\n",
    "\n",
    "# Exame usage\n",
    "folder_path = 'data/page_lines'\n",
    "all_files = get_all_files(folder_path)\n",
    "all_files.sort()\n",
    "for file in all_files:\n",
    "    print(infer_text(file))"
   ],
   "id": "7e535c2b4cabbd67",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amaljoe/Desktop/Workspace/IITB/NLP/OCR_with_LLMs/.venv/lib/python3.9/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not taking over the attendance .\n",
      "of entire college into our custody .\n",
      "hence there will be less assistance .\n",
      "Only the teacher's who wants\n",
      "to use our app needs to use .\n",
      "# your app . \" Over they can\n",
      "continue what they are\n",
      "doing currently . The end\n",
      "result at both is an\n",
      "excel sheet of attendance .\n",
      "I need more insight on his ,\n",
      "may have to ask Just a\n",
      "miss ) ( it not , we can\n",
      "change the end result to\n",
      "anything , pot , word , doesn't\n",
      "matter ) .\n",
      "- A single teacher run set up .\n",
      "the app for themselves since\n",
      "singer and class has are\n",
      "net required beforehand so\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
