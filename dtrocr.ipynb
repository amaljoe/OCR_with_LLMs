{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-07T12:04:13.754866Z",
     "start_time": "2024-11-07T12:04:00.004741Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Model, GPT2Config\n",
    "\n",
    "# Define patch embedding module\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=(224, 224), patch_size=(8, 4), embed_dim=768):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n",
    "        self.proj = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert the image to patches\n",
    "        x = self.proj(x)  # Shape: (batch_size, embed_dim, num_patches_height, num_patches_width)\n",
    "        x = x.flatten(2).transpose(1, 2)  # Shape: (batch_size, num_patches, embed_dim)\n",
    "        x = x + self.position_embeddings  # Add positional encoding\n",
    "        return x\n",
    "\n",
    "# Define DTrOCR model class\n",
    "class DTrOCR(nn.Module):\n",
    "    def __init__(self, embed_dim=768, max_seq_len=2568):\n",
    "        super(DTrOCR, self).__init__()\n",
    "        self.patch_embedding = PatchEmbedding(embed_dim=embed_dim)\n",
    "\n",
    "        # Load a pre-trained GPT-2 model as the decoder-only Transformer\n",
    "        config = GPT2Config(vocab_size=50257, n_positions=max_seq_len, n_embd=embed_dim, n_layer=12, n_head=12)\n",
    "        self.decoder = GPT2Model(config)\n",
    "\n",
    "        # Additional tokens for OCR\n",
    "        self.sep_token = torch.tensor([50256])  # Example special token (you may need to adjust)\n",
    "        self.eos_token = torch.tensor([50257])  # End of sequence token\n",
    "\n",
    "        # Output layer for generating token probabilities\n",
    "        self.lm_head = nn.Linear(embed_dim, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, images, labels=None):\n",
    "        # Patch embedding\n",
    "        x = self.patch_embedding(images)  # Shape: (batch_size, num_patches, embed_dim)\n",
    "        \n",
    "        # Add [SEP] token to separate image sequence and text sequence\n",
    "        # sep_token = self.sep_token.repeat(x.size(0), x.size(1), 1)\n",
    "        # x = torch.cat((x, sep_token), dim=2)\n",
    "\n",
    "        # Pass through GPT-2 decoder model\n",
    "        outputs = self.decoder(inputs_embeds=x)\n",
    "        logits = self.lm_head(outputs.last_hidden_state)\n",
    "        \n",
    "        print(logits.shape)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def generate_text(self, images, max_length=50):\n",
    "        # Start with the patch embeddings for the image and the [SEP] token\n",
    "        x = self.patch_embedding(images)\n",
    "        # sep_embed = self.sep_embedding.expand(x.size(0), -1, -1)\n",
    "        # x = torch.cat((x, sep_embed), dim=1)\n",
    "\n",
    "        generated_tokens = []\n",
    "\n",
    "        for i in range(max_length):\n",
    "            print(i)\n",
    "            # Get the decoder output logits for the current sequence\n",
    "            outputs = self.decoder(inputs_embeds=x)\n",
    "            logits = self.lm_head(outputs.last_hidden_state)\n",
    "\n",
    "            # Select the last token's logits and get the most likely next token\n",
    "            next_token_logits = logits[:, -1, :]  # Shape: (batch_size, vocab_size)\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1)  # Shape: (batch_size,)\n",
    "\n",
    "            # Append the token to the generated sequence\n",
    "            generated_tokens.append(next_token)\n",
    "\n",
    "            # Break if the EOS token is generated\n",
    "            if next_token.item() == self.eos_token.item():\n",
    "                break\n",
    "\n",
    "            # Update `x` by appending the embedding of the next token\n",
    "            next_token_embed = self.decoder.wte(next_token).unsqueeze(1)  # Embed the token\n",
    "            x = torch.cat((x, next_token_embed), dim=1)  # Append to the sequence\n",
    "\n",
    "        # Convert list of tokens to a tensor and return\n",
    "        return torch.stack(generated_tokens, dim=1)\n",
    "\n",
    "# Model instantiation\n",
    "model = DTrOCR()\n",
    "\n",
    "# Example inputs\n",
    "images = torch.randn(1, 3, 224, 224)  # Batch of 2 images, each 224x224 RGB\n",
    "labels = torch.randint(0, 50257, (1, 20))  # Example labels (batch of 2 sequences)\n",
    "\n",
    "# Generate text\n",
    "output = model.generate_text(images, max_length=5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T12:06:37.819830Z",
     "start_time": "2024-11-07T12:06:37.501279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.decode(output[0].tolist())"
   ],
   "id": "87ec094083f02e4e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 1989kee unrestricted routersograph'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T11:44:02.927385Z",
     "start_time": "2024-11-07T11:43:35.798899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ],
   "id": "4070f6cab0d40992",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T11:45:54.591560Z",
     "start_time": "2024-11-07T11:45:54.587052Z"
    }
   },
   "cell_type": "code",
   "source": "encoded_input.shape",
   "id": "22bf86b4d3bc1175",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[3041, 5372,  502,  416,  597, 2420,  345, 1549,  588,   13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
