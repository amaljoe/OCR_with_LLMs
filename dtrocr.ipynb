{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:01:24.101674Z",
     "start_time": "2024-11-12T13:01:21.108395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from IPython.utils.tokenutil import generate_tokens_catch_errors\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import torch\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "id = tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "model.wte(torch.tensor(id).unsqueeze(0)).shape, id, torch.tensor(tokenizer.convert_tokens_to_ids('[SEP]'))"
   ],
   "id": "c6bb7f628b30fd73",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amaljoe/Desktop/Workspace/IITB/NLP/OCR_with_LLMs/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 768]), 50256, tensor(50256))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:06:48.973697Z",
     "start_time": "2024-11-12T13:06:48.968794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input = \"Hello there,\"\n",
    "inputs = tokenizer(input, return_tensors=\"pt\")\n",
    "inputs"
   ],
   "id": "be038e178f8e7cd6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[15496,   612,    11]]), 'attention_mask': tensor([[1, 1, 1]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:12:38.309311Z",
     "start_time": "2024-11-12T13:12:37.656988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "gen_tokens = model.generate(\n",
    "    **inputs,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    max_length=10,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "output_scores=True,\n",
    "return_dict_in_generate=True\n",
    ")\n",
    "len(gen_tokens[\"scores\"]), gen_tokens[\"scores\"][0].shape"
   ],
   "id": "f801e6fcfb3b15d5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, torch.Size([1, 50257]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "gen_text = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0]\n",
    "gen_text"
   ],
   "id": "6073d8b95099f3f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sep_emb = model.wte(torch.tensor(id).unsqueeze(0))\n",
    "x = torch.rand(2, 196, 768)\n",
    "sep_emb = sep_emb.expand(x.size(0), -1, -1)\n",
    "sep_emb.shape"
   ],
   "id": "60f751852eabb1d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T09:32:58.233612Z",
     "start_time": "2024-11-11T09:32:58.226385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.cat((x, sep_emb), dim=1)\n",
    "x.shape"
   ],
   "id": "e556c5fb3a90f072",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 197, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T09:35:25.158626Z",
     "start_time": "2024-11-11T09:35:25.012796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "outputs = model(inputs_embeds=x)\n",
    "outputs.last_hidden_state.shape"
   ],
   "id": "2b268c6292eb5334",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 197, 768])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T09:38:14.902860Z",
     "start_time": "2024-11-11T09:38:14.773851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Output layer for generating token probabilities\n",
    "lm_head = nn.Linear(768, 50257, bias=False)\n",
    "\n",
    "logits = lm_head(outputs.last_hidden_state)\n",
    "logits[0, -1, :10]"
   ],
   "id": "d98b591891b60941",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1123, -1.1812, -0.7510,  0.2812, -0.9772, -0.0641, -1.6739, -0.0476,\n",
       "        -0.0111, -1.2077], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T09:55:56.595782Z",
     "start_time": "2024-11-11T09:55:56.591305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Select the last token's logits and get the most likely next token\n",
    "next_token_logits = logits[:, -1, :]  # Shape: (batch_size, vocab_size)\n",
    "next_token_logits.shape"
   ],
   "id": "8e63419a8023669a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 50257])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T09:49:14.293747Z",
     "start_time": "2024-11-11T09:49:14.289003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(1)\n",
    "next_token.shape"
   ],
   "id": "c7e4eb4bc7be6eec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T09:56:04.646648Z",
     "start_time": "2024-11-11T09:56:04.644521Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3b980dea02cbb8f6",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T09:56:34.735055Z",
     "start_time": "2024-11-11T09:56:34.729986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "next_token_logits = logits[:, -1, :]  # Shape: (batch_size, vocab_size)\n",
    "next_token_logits = next_token_logits.unsqueeze(1)\n",
    "\n",
    "generated_tokens = None\n",
    "for i in range(3):\n",
    "    if generated_tokens is None:\n",
    "        generated_tokens = next_token_logits\n",
    "    else:\n",
    "        generated_tokens = torch.hstack((generated_tokens, next_token_logits))\n",
    "generated_tokens.shape"
   ],
   "id": "6f93c94e634c6ea5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 50257])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T10:25:23.106205Z",
     "start_time": "2024-11-11T10:25:19.376717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "ds = load_dataset(\"alpayariyak/IAM_Sentences\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "ds = ds[\"train\"].select(range(12))\n",
    "ds = ds.map(lambda x: {\"image\": transform(x[\"image\"]), \"text\": x[\"text\"]})\n",
    "\n",
    "ds"
   ],
   "id": "5827441e68f418f6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85990567889843ae8714c9e67805b6ee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'text'],\n",
       "    num_rows: 12\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T10:31:35.134145Z",
     "start_time": "2024-11-11T10:31:32.254353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "ds2 = load_dataset(\"alpayariyak/IAM_Sentences\", split=\"train\")\n",
    "\n",
    "# Custom dataset class\n",
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, dataset, transform, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.dataset[idx][\"image\"]\n",
    "        if isinstance(image, list):\n",
    "            image = torch.tensor(image)\n",
    "        image = self.transform(image).to(\"mps\")\n",
    "        text = self.dataset[idx][\"text\"]\n",
    "        labels = self.tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", max_length=20, truncation=True)[\"input_ids\"].squeeze(0).to(\"mps\")\n",
    "        return image, labels\n",
    "\n",
    "# Create dataset and dataloader\n",
    "ocr_dataset = OCRDataset(ds2, transform, tokenizer)\n",
    "dataloader = DataLoader(ocr_dataset, batch_size=3, shuffle=True)\n",
    "\n",
    "# Example batch\n",
    "images, labels = next(iter(dataloader))\n",
    "images.shape, labels.shape"
   ],
   "id": "8ddb8533a423c03b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 3, 224, 224]), torch.Size([3, 20]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T10:04:51.683594Z",
     "start_time": "2024-11-11T10:04:51.671533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "avg = 0\n",
    "maximum = 0\n",
    "for i in range(100):\n",
    "    l = len(ds[\"text\"][i].split())\n",
    "    avg += l\n",
    "    if l > maximum:\n",
    "        maximum = l\n",
    "        \n",
    "avg /= 100\n",
    "avg, maximum"
   ],
   "id": "16e748d266cb597c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17.83, 35)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T10:33:43.100479Z",
     "start_time": "2024-11-11T10:33:40.513104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Model, GPT2Config, GPT2Tokenizer\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Define patch embedding module\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=(224, 224), patch_size=(16, 16), embed_dim=768):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n",
    "        self.patch_dim = patch_size[0] * patch_size[1] * 3  # 3 for RGB channels\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim))\n",
    "        self.proj = nn.Linear(self.patch_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        patches = x.unfold(2, self.patch_size[0], self.patch_size[0]).unfold(3, self.patch_size[1], self.patch_size[1])\n",
    "        patches = patches.contiguous().view(batch_size, channels, -1, self.patch_size[0] * self.patch_size[1])\n",
    "        patches = patches.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.patch_dim)\n",
    "        x = self.proj(patches)  # Project patches to embedding dimension\n",
    "        x = x + self.position_embeddings  # Add positional encoding\n",
    "        return x\n",
    "\n",
    "# Define DTrOCR model class\n",
    "class DTrOCR(nn.Module):\n",
    "    def __init__(self, embed_dim=768, max_seq_len=250):\n",
    "        super(DTrOCR, self).__init__()\n",
    "        self.patch_embedding = PatchEmbedding(embed_dim=embed_dim).to(\"mps\")\n",
    "\n",
    "        # Load a pre-trained GPT-2 model as the decoder-only Transformer\n",
    "        config = GPT2Config(vocab_size=50257, n_positions=max_seq_len, n_embd=embed_dim, n_layer=12, n_head=12)\n",
    "        self.decoder = GPT2Model(config)\n",
    "        # Define special tokens\n",
    "        self.sep_token = torch.tensor(tokenizer.convert_tokens_to_ids('[SEP]'))\n",
    "        # print(self.sep_token)\n",
    "        self.eos_token = torch.tensor(tokenizer.convert_tokens_to_ids('[EOS]'))\n",
    "\n",
    "        # Output layer for generating token probabilities\n",
    "        self.lm_head = nn.Linear(embed_dim, config.vocab_size, bias=False)\n",
    "        \n",
    "    def forward(self, images, max_length=50):\n",
    "        # Start with the patch embeddings for the image and the [SEP] token\n",
    "        x = self.patch_embedding(images)\n",
    "        \n",
    "    \n",
    "        # Append the [SEP] token embedding to the sequence\n",
    "        sep_token_embed = self.decoder.wte(self.sep_token).unsqueeze(0).unsqueeze(0)\n",
    "        sep_token_embed = sep_token_embed.expand(x.size(0), -1, -1)  # Expand for batch size\n",
    "        x = torch.cat((x, sep_token_embed), dim=1)\n",
    "    \n",
    "        generated_tokens = None\n",
    "    \n",
    "        for i in range(max_length):\n",
    "            # print(f'Iteration {i} of {max_length}. Input shape: {x.shape}')\n",
    "            # Get the decoder output logits for the current sequence\n",
    "            outputs = self.decoder(inputs_embeds=x)\n",
    "            logits = self.lm_head(outputs.last_hidden_state)\n",
    "    \n",
    "            # Select the last token's logits and get the most likely next token\n",
    "            next_token_logits = logits[:, -1, :]  # Shape: (batch_size, vocab_size)\n",
    "            \n",
    "            \n",
    "            next_token = torch.argmax(next_token_logits, dim=-1)  # Shape: (batch_size,)\n",
    "            next_token_logits = next_token_logits.unsqueeze(1)\n",
    "            \n",
    "            \n",
    "            if generated_tokens is None:\n",
    "                generated_tokens = next_token_logits\n",
    "            else:\n",
    "                generated_tokens = torch.hstack((generated_tokens, next_token_logits))\n",
    "\n",
    "\n",
    "            # Update `x` by appending the embedding of the next token\n",
    "            next_token_embed = self.decoder.wte(next_token).unsqueeze(1)  # Embed the token\n",
    "            x = torch.cat((x, next_token_embed), dim=1)  # Append to the sequence\n",
    "    \n",
    "            # Debugging: Print shapes and types\n",
    "            # print(f'Next token: {next_token}')\n",
    "            # print(f'Next token embed shape: {next_token_embed.shape}')\n",
    "            # print(f'Updated input shape: {x.shape}')\n",
    "    \n",
    "        return generated_tokens\n",
    "\n",
    "def train(model, dataloader, epochs=5, lr=1e-4):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss_value = 0\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images, labels\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, max_length=labels.shape[1]).to(\"mps\")\n",
    "            outputs = outputs.permute(0, 2, 1)\n",
    "            # print(\"Output shape, labels shape:\", outputs.shape, labels.shape)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_value = loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss_value}\")\n",
    "\n",
    "# Instantiate and train the model\n",
    "model = DTrOCR()\n",
    "# test = torch.rand(2, 3, 224, 224)\n",
    "# model(test, max_length=3)\n",
    "train(model, dataloader)"
   ],
   "id": "a0f71389d78ddfab",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): all input tensors must be on the same device. Received mps:0 and cpu",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 110\u001B[0m\n\u001B[1;32m    107\u001B[0m model \u001B[38;5;241m=\u001B[39m DTrOCR()\n\u001B[1;32m    108\u001B[0m \u001B[38;5;66;03m# test = torch.rand(2, 3, 224, 224)\u001B[39;00m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;66;03m# model(test, max_length=3)\u001B[39;00m\n\u001B[0;32m--> 110\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[19], line 97\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, dataloader, epochs, lr)\u001B[0m\n\u001B[1;32m     94\u001B[0m images, labels \u001B[38;5;241m=\u001B[39m images, labels\n\u001B[1;32m     96\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 97\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmps\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     98\u001B[0m outputs \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     99\u001B[0m \u001B[38;5;66;03m# print(\"Output shape, labels shape:\", outputs.shape, labels.shape)\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/Workspace/IITB/NLP/OCR_with_LLMs/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Workspace/IITB/NLP/OCR_with_LLMs/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[19], line 51\u001B[0m, in \u001B[0;36mDTrOCR.forward\u001B[0;34m(self, images, max_length)\u001B[0m\n\u001B[1;32m     49\u001B[0m sep_token_embed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder\u001B[38;5;241m.\u001B[39mwte(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msep_token)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     50\u001B[0m sep_token_embed \u001B[38;5;241m=\u001B[39m sep_token_embed\u001B[38;5;241m.\u001B[39mexpand(x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# Expand for batch size\u001B[39;00m\n\u001B[0;32m---> 51\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msep_token_embed\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     53\u001B[0m generated_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(max_length):\n\u001B[1;32m     56\u001B[0m     \u001B[38;5;66;03m# print(f'Iteration {i} of {max_length}. Input shape: {x.shape}')\u001B[39;00m\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;66;03m# Get the decoder output logits for the current sequence\u001B[39;00m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: torch.cat(): all input tensors must be on the same device. Received mps:0 and cpu"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T10:08:06.312514Z",
     "start_time": "2024-11-11T10:08:06.309165Z"
    }
   },
   "cell_type": "code",
   "source": "torch.mps.empty_cache()",
   "id": "b3aae5f82d55bce4",
   "outputs": [],
   "execution_count": 110
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # Model instantiation\n",
    "# model = DTrOCR().to(\"mps\")\n",
    "# \n",
    "# # Example inputs\n",
    "# images = torch.tensor(ds[\"image\"][0]).unsqueeze(0).to(\"mps\")\n",
    "# labels = tokenizer(ds[\"text\"][0], return_tensors=\"pt\")[\"input_ids\"].to(\"mps\")\n",
    "# labels = torch.tensor(labels)\n",
    "# \n",
    "# # Generate text\n",
    "# output = model.forward(images, max_length=3)\n",
    "# tokenizer.decode(output[0].tolist())"
   ],
   "id": "484f0b43d2400ae7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T11:42:18.720065Z",
     "start_time": "2024-11-10T11:42:18.691354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "r1 = np.array([1, 1])\n",
    "r2 = np.array([2, 2])\n",
    "r3 = np.array([3, 3])\n",
    "\n",
    "torch.stack([r1, r2, r3], dim=0)"
   ],
   "id": "dcdbe73c389ac5ac",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m r2 \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m2\u001B[39m])\n\u001B[1;32m      3\u001B[0m r3 \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m3\u001B[39m])\n\u001B[0;32m----> 5\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mr1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mr2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mr3\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: expected Tensor as element 0 in argument 0, but got numpy.ndarray"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T06:50:35.712882Z",
     "start_time": "2024-11-11T06:50:35.707376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "r1 = torch.tensor([1, 1, 1])\n",
    "r2 = torch.tensor([2, 2, 2])\n",
    "r3 = torch.tensor([3, 3, 3])\n",
    "\n",
    "r11 = torch.tensor([[1, 1, 1], [1, 1, 1]])\n",
    "r22 = torch.tensor([[2, 2, 2], [2, 2, 2]])\n",
    "r33 = torch.tensor([[3, 3, 3], [3, 3, 3]])\n",
    "\n",
    "torch.stack([r11, r22, r33], dim=0)"
   ],
   "id": "6fed3114b1a85428",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1],\n",
       "         [1, 1, 1]],\n",
       "\n",
       "        [[2, 2, 2],\n",
       "         [2, 2, 2]],\n",
       "\n",
       "        [[3, 3, 3],\n",
       "         [3, 3, 3]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
