{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:59:20.969886Z",
     "start_time": "2024-11-26T06:59:02.221357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer, VisionEncoderDecoderModel, TrOCRProcessor, AutoTokenizer\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\n",
    "trocr_model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten').to(device)"
   ],
   "id": "bcfa2b20e9d0e239",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amaljoe/Desktop/Workspace/IITB/NLP/OCR_with_LLMs/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": false,\n",
      "  \"transformers_version\": \"4.46.2\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 1024,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"transformers_version\": \"4.46.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-large-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T07:37:42.765009Z",
     "start_time": "2024-11-26T07:37:40.448045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "roberta_model = AutoModelForCausalLM.from_pretrained(\"FacebookAI/roberta-base\", is_decoder=True).to(device)"
   ],
   "id": "ca267809519217ca",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:59:23.057906Z",
     "start_time": "2024-11-26T06:59:23.036742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "\n",
    "# create ids of encoded input vectors\n",
    "filepath = 'data/ss2.png'\n",
    "image = Image.open(filepath).convert(\"RGB\")\n",
    "pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(device)"
   ],
   "id": "c76885bf34c291ff",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1f0f7ea31977a92d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T07:01:19.510880Z",
     "start_time": "2024-11-26T07:01:19.497189Z"
    }
   },
   "cell_type": "code",
   "source": "# create BOS token\n",
   "id": "3ccd3fe3f77a064c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 100,   64,  422,   24,  423,    4,  978, 1437]], device='mps:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T07:38:37.663341Z",
     "start_time": "2024-11-26T07:38:37.572993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#roberta loop\n",
    "\n",
    "decoder_input_ids = tokenizer(\"Capital of France is \", add_special_tokens=False, return_tensors=\"pt\").input_ids.to(device)\n",
    "decoder_input_ids\n",
    "\n",
    "lm_logits = roberta_model(input_ids=decoder_input_ids, return_dict=True).logits\n",
    "\n",
    "# sample last token with highest prob again\n",
    "next_decoder_input_ids = torch.argmax(lm_logits[:, -1:], axis=-1)\n",
    "\n",
    "# concat again\n",
    "decoder_input_ids = torch.cat([decoder_input_ids, next_decoder_input_ids], axis=-1)\n",
    "tokenizer.decode(decoder_input_ids[0], skip_special_tokens=True), decoder_input_ids"
   ],
   "id": "5e942df256da4a0e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Capital of France is ',\n",
       " tensor([[38632,     9,  1470,    16,  1437,     2]], device='mps:0'))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T15:56:32.898883Z",
     "start_time": "2024-11-25T15:56:32.859182Z"
    }
   },
   "cell_type": "code",
   "source": "assert decoder_input_ids[0, 0].item() == trocr_model.config.decoder_start_token_id, \"`decoder_input_ids` should correspond to `model.config.decoder_start_token_id`\"",
   "id": "ba3711a994992614",
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "`decoder_input_ids` should correspond to `model.config.decoder_start_token_id`",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m decoder_input_ids[\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m==\u001B[39m trocr_model\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mdecoder_start_token_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`decoder_input_ids` should correspond to `model.config.decoder_start_token_id`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[0;31mAssertionError\u001B[0m: `decoder_input_ids` should correspond to `model.config.decoder_start_token_id`"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:10:51.245516Z",
     "start_time": "2024-11-26T06:10:49.981915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# STEP 1\n",
    "\n",
    "# pass input_ids to encoder and to decoder and pass BOS token to decoder to retrieve first logit\n",
    "outputs = trocr_model(pixel_values, decoder_input_ids=decoder_input_ids, return_dict=True)"
   ],
   "id": "a2346c7be8526a97",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:10:52.664195Z",
     "start_time": "2024-11-26T06:10:52.611795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# get encoded sequence\n",
    "encoded_sequence = (outputs.encoder_last_hidden_state,)\n",
    "# get logits\n",
    "lm_logits = outputs.logits\n",
    "\n",
    "# sample last token with highest prob\n",
    "next_decoder_input_ids = torch.argmax(lm_logits[:, -1:], axis=-1)\n",
    "\n",
    "next_decoder_input_ids"
   ],
   "id": "90025331d584fcaf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2]], device='mps:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:10:54.877420Z",
     "start_time": "2024-11-26T06:10:54.826206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# concat\n",
    "decoder_input_ids = torch.cat([decoder_input_ids, next_decoder_input_ids], axis=-1)\n",
    "decoder_input_ids"
   ],
   "id": "6392e05c374b3ade",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  100,    33,    57,  3044,    70,   363,     4,   978,    47,   646,\n",
       "         31957,   742,     2]], device='mps:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:10:57.953439Z",
     "start_time": "2024-11-26T06:10:57.943366Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(decoder_input_ids[0], skip_special_tokens=True)",
   "id": "e2d4a2db56232d61",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have been testing all night. Now you [bos]'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:11:11.405684Z",
     "start_time": "2024-11-26T06:11:11.325895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# STEP 2\n",
    "\n",
    "# reuse encoded_inputs and pass BOS + \"Ich\" to decoder to second logit\n",
    "lm_logits = trocr_model(None, encoder_outputs=encoded_sequence, decoder_input_ids=decoder_input_ids, return_dict=True).logits\n",
    "\n",
    "# sample last token with highest prob again\n",
    "next_decoder_input_ids = torch.argmax(lm_logits[:, -1:], axis=-1)\n",
    "\n",
    "# concat again\n",
    "decoder_input_ids = torch.cat([decoder_input_ids, next_decoder_input_ids], axis=-1)\n",
    "tokenizer.decode(decoder_input_ids[0], skip_special_tokens=True), decoder_input_ids"
   ],
   "id": "39ed9750ef5c507d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I have been testing all night. Now you [bos] get )',\n",
       " tensor([[  100,    33,    57,  3044,    70,   363,     4,   978,    47,   646,\n",
       "          31957,   742,     2,   120,  4839,     2,     2]], device='mps:0'))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T05:55:41.882258Z",
     "start_time": "2024-11-26T05:55:41.720609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# STEP 3\n",
    "lm_logits = trocr_model(None, encoder_outputs=encoded_sequence, decoder_input_ids=decoder_input_ids, return_dict=True).logits\n",
    "next_decoder_input_ids = torch.argmax(lm_logits[:, -1:], axis=-1)\n",
    "decoder_input_ids = torch.cat([decoder_input_ids, next_decoder_input_ids], axis=-1)\n",
    "\n",
    "# let's see what we have generated so far!\n",
    "print(f\"Generated so far: {tokenizer.decode(decoder_input_ids[0], skip_special_tokens=True)}\")\n",
    "\n",
    "# This can be written in a loop as well.\n"
   ],
   "id": "ed6ece80a4c09a09",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated so far: [bos] 5-10\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T07:44:11.540115Z",
     "start_time": "2024-11-26T07:44:11.537342Z"
    }
   },
   "cell_type": "code",
   "source": "trocr_model.config.num_beams = 2",
   "id": "de528bdb50f81e6b",
   "outputs": [],
   "execution_count": 115
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T07:47:30.261341Z",
     "start_time": "2024-11-26T07:47:28.972614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_model_output(images):\n",
    "    pixel_values = processor(images=images, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    output = trocr_model.generate(pixel_values, return_dict_in_generate=True, output_scores=True, max_new_tokens=30, output_logits=True)\n",
    "    \n",
    "    generated_texts = processor.batch_decode(output.sequences, skip_special_tokens=True)\n",
    "    return generated_texts, output.sequences_scores, output\n",
    "\n",
    "img = Image.open(\"data/fml_line.png\").convert(\"RGB\")\n",
    "_, _, output = get_model_output([img])"
   ],
   "id": "f5d5644cb4535917",
   "outputs": [],
   "execution_count": 132
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T08:16:12.203154Z",
     "start_time": "2024-11-26T08:16:12.196563Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.encode(\" cluster\")",
   "id": "c12320bf4df81777",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 18016, 2]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 146
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T08:37:42.873915Z",
     "start_time": "2024-11-26T08:37:42.842240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cluster_logit = None\n",
    "\n",
    "for logit in output.logits[2:3]:    \n",
    "    confidence = logit.softmax(-1).max()\n",
    "    cluster_logit = logit\n",
    "    word = tokenizer.decode([logit.argmax()])\n",
    "    print(word, confidence, logit.argmax())"
   ],
   "id": "c08182aef94b3763",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " controls tensor(0.4203, device='mps:0') tensor(5656, device='mps:0')\n"
     ]
    }
   ],
   "execution_count": 182
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "FML_text = \"\"\"K Means clustering algorithm\n",
    "Assume we have K cluster of points; each point in a cluster\n",
    "Is closest to its centroid (more than any other cluster centroid)\n",
    "If cluster assignment is known, it is easy to compute the centroid\"\"\"\n",
    "\n"
   ],
   "id": "775e09e2d00e3bd4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T09:19:29.052966Z",
     "start_time": "2024-11-26T09:19:22.821320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "unmasker_base = pipeline('fill-mask', model='roberta-base', device=\"mps\")\n",
    "unmasker_large = pipeline('fill-mask', model='roberta-large', device=\"mps\")"
   ],
   "id": "47255b9838839670",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amaljoe/Desktop/Workspace/IITB/NLP/OCR_with_LLMs/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T09:19:55.184281Z",
     "start_time": "2024-11-26T09:19:55.170767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cluster_pred = cluster_logit.softmax(-1)\n",
    "cluster_pred.shape"
   ],
   "id": "609bfaa0519f4a4c",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cluster_logit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m cluster_pred \u001B[38;5;241m=\u001B[39m \u001B[43mcluster_logit\u001B[49m\u001B[38;5;241m.\u001B[39msoftmax(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      2\u001B[0m cluster_pred\u001B[38;5;241m.\u001B[39mshape\n",
      "\u001B[0;31mNameError\u001B[0m: name 'cluster_logit' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T08:29:38.683532Z",
     "start_time": "2024-11-26T08:29:38.543195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "res = unmasker_base(\"\"\"K Means clustering algorithm\n",
    "Assume we have K cluster of points; each point in a cluster\n",
    "Is closest to its centroid (more than any other cluster centroid)\n",
    "If cluster assignment is known, it is easy to compute the centroid\n",
    "If <mask> centroid is known, it is easy to do cluster assignment\"\"\")\n",
    "\n",
    "\n",
    "res"
   ],
   "id": "652aafc339878f79",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.8191075921058655,\n",
       "  'token': 5,\n",
       "  'token_str': ' the',\n",
       "  'sequence': 'K Means clustering algorithm\\nAssume we have K cluster of points; each point in a cluster\\nIs closest to its centroid (more than any other cluster centroid)\\nIf cluster assignment is known, it is easy to compute the centroid\\nIf the centroid is known, it is easy to do cluster assignment'},\n",
       " {'score': 0.034218356013298035,\n",
       "  'token': 18016,\n",
       "  'token_str': ' cluster',\n",
       "  'sequence': 'K Means clustering algorithm\\nAssume we have K cluster of points; each point in a cluster\\nIs closest to its centroid (more than any other cluster centroid)\\nIf cluster assignment is known, it is easy to compute the centroid\\nIf cluster centroid is known, it is easy to do cluster assignment'},\n",
       " {'score': 0.01694742776453495,\n",
       "  'token': 117,\n",
       "  'token_str': ' no',\n",
       "  'sequence': 'K Means clustering algorithm\\nAssume we have K cluster of points; each point in a cluster\\nIs closest to its centroid (more than any other cluster centroid)\\nIf cluster assignment is known, it is easy to compute the centroid\\nIf no centroid is known, it is easy to do cluster assignment'},\n",
       " {'score': 0.010521210730075836,\n",
       "  'token': 63,\n",
       "  'token_str': ' its',\n",
       "  'sequence': 'K Means clustering algorithm\\nAssume we have K cluster of points; each point in a cluster\\nIs closest to its centroid (more than any other cluster centroid)\\nIf cluster assignment is known, it is easy to compute the centroid\\nIf its centroid is known, it is easy to do cluster assignment'},\n",
       " {'score': 0.007529187481850386,\n",
       "  'token': 10,\n",
       "  'token_str': ' a',\n",
       "  'sequence': 'K Means clustering algorithm\\nAssume we have K cluster of points; each point in a cluster\\nIs closest to its centroid (more than any other cluster centroid)\\nIf cluster assignment is known, it is easy to compute the centroid\\nIf a centroid is known, it is easy to do cluster assignment'}]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 165
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T09:41:08.404975Z",
     "start_time": "2024-11-26T09:41:07.764220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "res = unmasker_large(\"\"\"K Means clustering algorithm\n",
    "Assume we have K cluster of points; each point in a cluster\n",
    "Is closest to its centroid (more than any other cluster centroid)\n",
    "If cluster assignment is known, it is easy to compute the centroid\n",
    "If cluster <mask> is known, it is easy to do cluster assignment\"\"\")\n",
    "\n",
    "for pred in res:\n",
    "    score, token, str = pred['score'], pred['token'], pred['token_str']\n",
    "    confidence = score + max(cluster_pred[0][token], cluster_pred[1][token])\n",
    "    print(str, confidence, max(cluster_pred[0][token], cluster_pred[1][token]))"
   ],
   "id": "91a529f4bfd462e2",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cluster_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 9\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m pred \u001B[38;5;129;01min\u001B[39;00m res:\n\u001B[1;32m      8\u001B[0m     score, token, \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m pred[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mscore\u001B[39m\u001B[38;5;124m'\u001B[39m], pred[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtoken\u001B[39m\u001B[38;5;124m'\u001B[39m], pred[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtoken_str\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m----> 9\u001B[0m     confidence \u001B[38;5;241m=\u001B[39m score \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mmax\u001B[39m(\u001B[43mcluster_pred\u001B[49m[\u001B[38;5;241m0\u001B[39m][token], cluster_pred[\u001B[38;5;241m1\u001B[39m][token])\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mstr\u001B[39m, confidence, \u001B[38;5;28mmax\u001B[39m(cluster_pred[\u001B[38;5;241m0\u001B[39m][token], cluster_pred[\u001B[38;5;241m1\u001B[39m][token]))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'cluster_pred' is not defined"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T11:47:22.784458Z",
     "start_time": "2024-11-26T11:47:21.669614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Input text with a mask token\n",
    "text = \"K Means clustering algorithm. <mask> we have K cluster of <mask><s>; each point in a cluster.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Get the logits from the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Get the logits for the masked token\n",
    "mask_token_index = torch.where(inputs.input_ids == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = logits[0, mask_token_index, :]\n",
    "\n",
    "# Print the top 5 tokens predicted for the masked token\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "# for token in top_5_tokens:\n",
    "#     print(tokenizer.decode([token]))\n",
    "\n",
    "print(tokenizer.decode(logits.argmax(-1)[0], skip_special_tokens=True), inputs)"
   ],
   "id": "2ace3d8529bc2bd4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel clustering algorithm. So we have K cluster of points; each point in a cluster. {'input_ids': tensor([[    0,   530, 27088, 46644,  2961, 17194,     4, 50264,    52,    33,\n",
      "           229, 18016,     9, 50264,     0,   131,   349,   477,    11,    10,\n",
      "         18016,     4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
